{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.linear_model import SGDClassifier\n\nimport tensorflow as tf\nfrom tensorflow.keras.optimizers.schedules import PolynomialDecay\n\nfrom transformers import AutoTokenizer, DataCollatorWithPadding\nfrom transformers import TFAutoModelForSequenceClassification\nfrom transformers import AdamWeightDecay\nfrom datasets import Dataset, DatasetDict\n\nprint(tf.__version__)\nprint(tf.config.list_physical_devices())\n\n# startegy for training on multiple gpus\nmirrored_strategy = tf.distribute.MirroredStrategy()\n","metadata":{"execution":{"iopub.status.busy":"2023-05-18T12:58:34.811751Z","iopub.execute_input":"2023-05-18T12:58:34.812445Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"path = \"/kaggle/input/edos-1m/\"\n\ndataset = pd.read_csv(path + \"EDOS 1M.csv\")\n#dataset = dataset.head(10000)\n#classes = dataset[\"eb+_emot\"].unique()\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#preparing the new dataset containing utterances pairs\n\ndf = dataset.merge(dataset, on='dialogue_id', how='inner') #self join\n#creating auxialiary attributes\ndf['is_first'] = (df['turn_x'] == 1) & (df['turn_y'] == 1)\ndf['is_last'] = (df['turn_x'] == df['turn_y']) & (df['turn_y'] == df.groupby('dialogue_id')['turn_y'].transform(max))\n#keep only first/last utterances and all the consecutive pairs               \ndf = df[df['is_first'] | df['is_last'] | (df['turn_x'] == df['turn_y'] - 1)]\n#display(df,10) \n\n#df_preceding will be used to predict the last utterance, given the previous context, if it exists\ndf_preceding = df[df['is_last'] == 0]\ndf_preceding = df_preceding[['dialogue_id','turn_x','uttr_x','turn_y','uttr_y','eb+_emot_y','is_first']].rename(columns={'eb+_emot_y': 'label'})\n#df_following will be used to predict the first utterance, given the following context, if it exists\ndf_following = df[df['is_first'] == 0]\ndf_following = df_following[['dialogue_id','turn_x','uttr_x','turn_y','uttr_y','eb+_emot_x','is_last']].rename(columns={'eb+_emot_x': 'label'})\n#replace utterances with empty strings for first and last samples of each conversation\ndf_preceding.loc[df_preceding['is_first'] == 1, 'uttr_x'] = \"\"\ndf_following.loc[df_following['is_last'] == 1, 'uttr_y'] = \"\"\n\ndisplay(df_preceding,10)\ndisplay(df_following,10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_choice = 0 #0 for preceding, 1 for following\n# train, validation and test split\nif df_choice == 0:\n    df = df_preceding\n    train_X, valid_X, train_y, valid_y = train_test_split(df[['dialogue_id','uttr_x','uttr_y','is_first']], df['label'], test_size=0.15, stratify= None, shuffle=False)\nelse:\n    df = df_following\n    train_X, valid_X, train_y, valid_y = train_test_split(df[['dialogue_id','uttr_x','uttr_y','is_last']], df['label'], test_size=0.15, stratify= None, shuffle=False)\n\nclasses = df['label'].unique()\nprint(len(classes))\n\ndisplay(valid_X,10)\nprint(\"train size: \", len(train_X))\nprint(\"validation size: \", len(valid_X))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model metadata\nmodel_name = \"distilbert-base-uncased\"\n# map expected ids to their labels and viceversa\nid2label = dict(zip(range(len(classes)), classes))\nlabel2id = dict(zip(classes, range(len(classes))))\nid2label\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# building the datasets\nif df_choice == 0:\n    flag = \"is_first\"\nelse:\n    flag= \"is_last\"\ntrain_data = Dataset.from_pandas(pd.DataFrame({\"text_1\": train_X['uttr_x'],\"text_2\": train_X['uttr_y'], flag: train_X[flag], \"label\": np.argmax(pd.get_dummies(train_y).to_numpy(), axis=1)}), preserve_index=False)\nvalid_data = Dataset.from_pandas(pd.DataFrame({\"text_1\": valid_X['uttr_x'],\"text_2\": valid_X['uttr_y'], flag: valid_X[flag], \"label\": np.argmax(pd.get_dummies(valid_y).to_numpy(), axis=1)}), preserve_index=False)\n\n# shuffling is performed at the previous operation -> we need to redefine valid_y\nvalid_y = valid_data['label']\n\ndata = DatasetDict()\ndata['train'] = train_data\ndata['validation'] = valid_data\n\nprint(data['train'][0])\ndata","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load the tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# tokenize the data\ndef preprocess_function(examples):\n    return tokenizer(examples[\"text_1\"],examples[\"text_2\"], truncation=True)\n\ncols = data[\"train\"].column_names\ntokenized_data = data.map(preprocess_function, batched=True, remove_columns=[\"text_1\",\"text_2\",flag])\n#okkk\nprint(tokenized_data[\"train\"][0])\ntokenized_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# metadata\nbatch_size = 128\nnum_epochs = 15\npatience = 3\nonly_fine_tune = True\n\n# convert datasets to a suitable format for tensorflow\ndata_collator = DataCollatorWithPadding(tokenizer, return_tensors=\"tf\")\n\ntf_train_dataset = tokenized_data[\"train\"].to_tf_dataset(\n    columns=[\"attention_mask\", \"input_ids\", \"label\"],\n    shuffle=True,\n    batch_size=batch_size,\n    collate_fn=data_collator,\n)\n\ntf_validation_dataset = tokenized_data[\"validation\"].to_tf_dataset(\n    columns=[\"attention_mask\", \"input_ids\", \"label\"],\n    shuffle=False,\n    batch_size=batch_size,\n    collate_fn=data_collator,\n)\n\n# create optimizer and learning rate scheduler\nnum_train_steps = len(tf_train_dataset) * num_epochs\nlr_scheduler = PolynomialDecay(\n    initial_learning_rate=5e-5, end_learning_rate=0.0, decay_steps=num_train_steps\n)\nopt = AdamWeightDecay(learning_rate=lr_scheduler,\n                      weight_decay_rate=0.01)\n\n# from within the selected parallelization strategy...\nwith mirrored_strategy.scope():\n    \n    # ...load the model...\n    model = TFAutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(classes), id2label=id2label, label2id=label2id)\n    if only_fine_tune:\n        for i in range(1):\n            model.layers[i].trainable = False\n    \n    # ...and compile it\n    model.compile(optimizer=opt,\n                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n                  metrics=[\"accuracy\"])\n\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# training\nhistory = model.fit(\n          tf_train_dataset,\n          validation_data=tf_validation_dataset,\n          epochs=num_epochs,                                  \n          callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', patience=patience, restore_best_weights=True)]\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# validate the model -> accuracy should correspond to final val_accuracy\nbert_y = np.argmax(model.predict(tf_validation_dataset)[\"logits\"], axis=1)\n\nprint('Results for BERT-based classifier:')\nprint(classification_report(valid_y, bert_y, target_names=classes))","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}